"""

File description:

- This code is for training a NN as a REGRESSOR for BER estimation in dB scale. The ultimate goal is to have
5 output nodes (for each mitigation type) and 9 input features (the meta data). Right now the code runs
with 5 input features and 2 outputs (unmitigated BER and FFT-based mitigated BER).

- The code has no input. All it needs is the path to the data (line 54 and 56). It reads .mat files, as the
data was generated by Matlab. All the .mat files must be kept in the folder named 'Data'.

- The code outputs:

1) It prints the mean absolute error (MAE) on the test data for both mitigated BER and unmitigated BER.
2) It prints the correlation coefficient (corr) between the true test labels and the predicted test labels,
 for both mitigated BER and unmitigated BER.
3) It saves  the scatter plot between the true labels and the predicted labels for both unmitigated and
mitigated BER. If the network is learning well, the scatter plot should be close to the line (x=y).
4) It also saves the model and its weights.

Note: All the saved files are in a folder named 'Results'.

"""


import time
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
import numpy as np
import hdf5storage # load mat files
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense,Flatten, Lambda
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt
from datetime import datetime
import scipy.io as sio
import random

os.environ["CUDA_VISIBLE_DEVICES"] = "1"

np.random.seed(0)
tf.random.set_seed(0)

# ======================== Parameters ========================

dataID = './DATA/'
epochs = 100   # number of learning epochs
batch_size = 32
early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, mode='auto') # Early stopping
hidden_neurons = 256
#  =============== load data =================================

t = hdf5storage.loadmat(dataID+'X16k1.mat')  # XLong1    XScale1
X = t['X']
t = hdf5storage.loadmat(dataID+'Y16k1.mat')  #  YLong1   YScale1
Y= t['Y']

# ================== Data processing ###################

# Ber in dB scale
Y = - 10*(np.log(Y/1000)/np.log(10))   # log value
for i in range(0,Y.shape[0]):
    for j in range(0,2):
        if np.isinf(Y[i,j]) or np.isnan(Y[i,j]) or Y[i,j] > 40 :
            Y[i,j] = 40

# split data
train_fraction = 0.7
train_size = int(train_fraction*Y.shape[0])
Xtrain = X[:train_size,:]
Ytrain = Y[:train_size,:]

val_fraction = 0.5
val_size = int((Y.shape[0] - train_size)*val_fraction)
Xval = X[train_size:train_size+val_size, :]
Yval = Y[train_size:train_size+val_size,:]

Xtest = X[train_size+val_size:,:]
Ytest = Y[train_size+val_size:,:]

# extend dim
Xtrain = Xtrain[:, :, np.newaxis]
Xtest = Xtest[:, :, np.newaxis]
Xval = Xval[:, :, np.newaxis]


#================ Model Building ===========================

nn_input  = Input((Xtrain.shape[1],1))
nn_output = Flatten()(nn_input)
nn_output = Dense(hidden_neurons,activation='relu')(nn_output)
nn_output = Dense(hidden_neurons,activation='relu')(nn_output)
nn_output = Dense(hidden_neurons,activation='relu')(nn_output)
nn_output = Dense(Ytrain.shape[1],activation='linear')(nn_output)

nn = Model(inputs=nn_input,outputs=nn_output)

#================ Model Compiling ===========================

optz = tf.keras.optimizers.Adam(lr=0.0001,beta_1=0.9, beta_2=0.999, amsgrad=False)
nn.compile(optimizer=optz, loss='mse',metrics=['mse'])
nn.summary()
train_hist = nn.fit(x=Xtrain,y=Ytrain,\
                    batch_size = batch_size ,epochs = epochs ,\
                    validation_data=(Xval, Yval), shuffle=True, callbacks=[early_stop])

# ================ Evaluate Model  ===========================

# learning curve
plt.figure(1)
plt.plot(train_hist.history['mse'])
plt.plot(train_hist.history['val_mse'])
plt.title('distance')
plt.ylabel('distance')
plt.xlabel('epoch')
plt.grid(True)
plt.legend(['train', 'validate'])
plt.savefig('./Results/hist_dist.png')


Ypredtrain = nn.predict(Xtrain)
Ypred = nn.predict(Xtest)
err_unmit = np.mean(abs(Ytest[:,0]-Ypred[:,0]))
print( "--- MAE_unmit: --- %s" %(err_unmit))
corr_unmit = np.corrcoef(Ytest[:, 0], Ypred[:, 0])[0][1]
print("--- Corr_unmit: --- %s"% corr_unmit)

err_mit = np.mean(abs(Ytest[:,1]-Ypred[:,1]))
print( "--- MAE_mit: --- %s" %(err_mit))
corr_mit = np.corrcoef(Ytest[:, 1], Ypred[:, 1])[0][1]
print("--- Corr_mit: --- %s"% corr_mit)


# Histogramm of errors on test Data
# plt.figure(2)
# plt.hist(abs(Ytest - Ypred), bins=64)
# plt.ylabel('Number of occurence')
# plt.xlabel('Estimate error')
# plt.grid(True)
# plt.title('histogram of estimation error ')
# plt.savefig('./Results/hist_error.png')
#
# plt.figure(3)
# plt.hist(abs(Ytest[:, 1] - Ypred[:, 1]), bins=64)
# plt.ylabel('Number of occurence')
# plt.xlabel('Estimate error')
# plt.grid(True)
# plt.title('histogram of estimation error mit.')
# plt.savefig('./Results/hist_error_mit.png')

# scatter plot of Y_true VS Y_pred
plt.figure(4)
plt.scatter(Ytest[:,0], Ypred[:,0], facecolors='none', edgecolors='b')
plt.title(' unmit est vs ground')
plt.ylabel('predicted')
plt.xlabel('True')
plt.grid(True)
plt.savefig('./Results/scatter_unmit.png')

plt.figure(5)
plt.scatter(Ytest[:,1], Ypred[:,1], facecolors='none', edgecolors='r')
plt.title(' mit est vs ground')
plt.ylabel('predicted')
plt.xlabel('True')
plt.grid(True)
plt.savefig('./Results/scatter_mit.png')


#============== save model to file =============
model_json = nn.to_json()
with open("./Results/model_TF.json", "w") as json_file:
    json_file.write(model_json)
    nn.save_weights("./Results/model_TF.h5")
    print("Saved model to disk")
